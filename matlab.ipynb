{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATLAB with Amazon SageMaker Example\n",
    "\n",
    "1. [Introduction](#1.-Introduction)\n",
    "\n",
    "2. [MATLAB on Amazon SageMaker](#2.-MATLAB-on-Amazon-SageMaker)\n",
    "\n",
    "3. [Prerequisites](#3.-Prerequisites)\n",
    "    1. [Roles, Permissions and Docker Service](#3.1-Roles,-Permissions-and-Docker-Service)\n",
    "    2. [License Manager for MATLAB](#3.2-License-Manager-for-MATLAB)\n",
    "    3. [Dockerfile & dependencies](#3.3-Dockerfile-&-dependencies)\n",
    "    \n",
    "4. [MATLAB docker image on ECR](#4.-MATLAB-docker-image-on-ECR)\n",
    "    1. [Create docker image from Dockerfile](#4.1-Create-docker-image-from-Dockerfile)\n",
    "    2. [Push MATLAB image to ECR](#4.2-Push-MATLAB-image-to-ECR)\n",
    "    \n",
    "5. [SageMaker processor](#5.-SageMaker-processor)\n",
    "    1. [Defining network configuration for Processing jobs](#5.1-Defining-network-configuration-for-Processing-jobs)\n",
    "    1. [Write the MATLAB script `main.m`](#5.2-Write-the-MATLAB-script)\n",
    "    2. [Running the docker processing container](#5.3-Running-the-docker-processing-container)\n",
    "    3. [Getting results back and printing accuracy](#5.4-Getting-results-back-from-docker-processing-container-to-SageMaker-instance)\n",
    "6. [Clean up](#6.-Cleaning-up-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction \n",
    "\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker/) helps data scientists and developers to prepare, build, train, and deploy high-quality machine learning (ML) models quickly by bringing together a broad set of capabilities purpose-built for ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MATLAB on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Amazon SageMaker, users can package their own algorithms that can then be trained and deployed in the SageMaker environment. This notebook will guide you through an example that shows you how to build a Docker container for SageMaker with MATLAB and use it for processing, training and inference.\n",
    "\n",
    "This notebook shows how you can:\n",
    "\n",
    "1. Build a docker container with MATLAB by using the [official docker hub MATLAB image](https://hub.docker.com/r/mathworks/matlab-deep-learning).  \n",
    "2. Publish the docker container to [Amazon ECR](https://aws.amazon.com/ecr/), from where SageMaker can use it to run processing jobs.\n",
    "3. Run a processing job on dataset for preprocessing, training and inference.\n",
    "4. Get results back from the processing job inside the SageMaker environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Roles, Permissions and Docker Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, import the Python libraries you'll need, and set up the environment with a few prerequisites for permissions and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beacuse you would be pulling the [matlab-deep-learning docker image](https://hub.docker.com/r/mathworks/matlab-deep-learning) (which has a compressed size of 7.89 GB), change the default docker location to the EBS volume mounted when creating the SageMaker notebook instance. \n",
    "\n",
    "The below commands stop the docker service, move the default docker directory from `/var/lib/docker` to `/home/ec2-user/Sagemaker/docker`, and then start the docker service again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/ec2-user/SageMaker/docker/\n",
    "!sudo service docker stop\n",
    "!sudo mv /var/lib/docker/ /home/ec2-user/SageMaker/docker/\n",
    "!sudo ln -s /home/ec2-user/SageMaker/docker/ /var/lib/docker\n",
    "!sudo service docker start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 License Manager for MATLAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the steps in the GitHub repo to launch tha License Manager for MATLAB on AWS - https://github.com/mathworks-ref-arch/license-manager-for-matlab-on-aws. \n",
    "\n",
    "The docker instance communicates with the License Manager for licensing via the `MLM_LICENSE_FILE` flag. So, note down the private IP Address of the License Manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dockerfile & dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p matlab-docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new `CMD` command which specifies the instruction to be executed when a Docker container starts. Create a dockerfile which pulls MATLAB's image from https://hub.docker.com/r/mathworks/matlab-deep-learning and adds a new CMD.\n",
    "\n",
    "In the Dockerfile, docker container runs the script `main` located in `/opt/ml/processing/src_files` when it starts. You create this script in section [Write the MATLAB script `main.m`](#5.2-Write-the-MATLAB-script) - which is then uploaded to the docker container in section [Running the docker processing container](#5.3-Running-the-docker-processing-container)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile matlab-docker/Dockerfile\n",
    "FROM mathworks/matlab-deep-learning\n",
    "USER root\n",
    "CMD [\"matlab\", \"-batch\", \"cd /opt/ml/processing/src_files; main; exit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo ==== Generated Dockerfile ====\n",
    "!cat matlab-docker/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MATLAB docker image on ECR  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Docker image with MATLAB needs to be available for SageMaker to use. \n",
    "\n",
    "The following steps:\n",
    "* Builds a MATLAB deep learning container from Dockerfile in section [Dockerfile & dependencies](#3.3-Dockerfile-&-dependencies) .\n",
    "* Creates an ECR Repo, and [pushes the container image](https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html) to it.\n",
    "\n",
    "These steps can be skipped if you already have a Docker Container image with MATLAB installed in an Amazon ECR repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "ecr_repository = 'sagemaker-demo-ecr' #ECR repository. which contains MATLAB deep learning container\n",
    "tag = ':latest'\n",
    "processing_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}'.format(account_id, region, ecr_repository + tag)\n",
    "\n",
    "print(\"ECR Repository Name: \", ecr_repository)\n",
    "print(\"ECR Repository URI:\", processing_repository_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create docker image from Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build docker image locally in SageMaker environment\n",
    "!docker build -t $ecr_repository matlab-docker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker image ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Push MATLAB image to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the ECR Repository\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "\n",
    "# Authorize Docker to publish to ECR\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account_id}.dkr.ecr.{region}.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push MATLAB image to ECR\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SageMaker processor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Defining network configuration for Processing jobs\n",
    "\n",
    "Follow this documentation from Amazon to give SageMaker Processing Jobs Access to Resources in your Amazon VPC - https://docs.aws.amazon.com/sagemaker/latest/dg/process-vpc.html. \n",
    "\n",
    "Make sure the same subnet is used to create Network license manager in section [License Manager for MATLAB](#3.2-License-Manager-for-MATLAB). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sagemaker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37/2007890518.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNetworkConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Network configuration documentation - https://sagemaker.readthedocs.io/en/stable/api/utility/network.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m processing_network_config = NetworkConfig(security_group_ids=[\"sg-abcdefg1234\"], \n\u001b[1;32m      5\u001b[0m                                           subnets=[\"subnet-abcdefg\"])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sagemaker'"
     ]
    }
   ],
   "source": [
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "# Network configuration documentation - https://sagemaker.readthedocs.io/en/stable/api/utility/network.html\n",
    "processing_network_config = NetworkConfig(security_group_ids=[\"sg-abcdefg1234\"], \n",
    "                                          subnets=[\"subnet-abcdefg\"])\n",
    "print(processing_network_config._to_request_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intialize the `sagemaker.processing.Processor` using the following arguments -\n",
    "\n",
    "- `image_uri`:  the processing_repository_uri defined above.\n",
    "-  `role`: the role defined above.\n",
    "- `instance_count`: number of instances to spawn in the job.\n",
    "- `instance_type`: the type of instance to spawn (For more information - https://aws.amazon.com/sagemaker/pricing/).\n",
    "- `network_config` : network configration for processing jobs  \n",
    "- `env`: environment variable to be passed to the docker instance. Includes `MLM_LICENSE_FILE` variable generated via the License Manager for MATLAB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sagemaker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37/410518352.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProcessingInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProcessingOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m processor = Processor(\n\u001b[1;32m      4\u001b[0m     \u001b[0mimage_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessing_repository_uri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrole\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sagemaker'"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "processor = Processor(\n",
    "    image_uri=processing_repository_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\", #replace with required instance\n",
    "    network_config = processing_network_config,\n",
    "    env = {\"MLM_LICENSE_FILE\":\"27000@111.222.333.444\"} #replace with your network license manager private IP address\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Write the MATLAB script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training code is written in the file `main.m`. It is inspired from MathWorks example - [Create Simple Deep Learning Network for Classification](https://www.mathworks.com/help/deeplearning/ug/create-simple-deep-learning-network-for-classification.html). \n",
    "\n",
    "Overview of the script - \n",
    "\n",
    "- loads the digit sample dataset as an [image datastore](https://www.mathworks.com/help/matlab/ref/matlab.io.datastore.imagedatastore.html).\n",
    "- splits the dataset into training & testing set.\n",
    "- define the convolutional neural network architecture.\n",
    "- specify training options.\n",
    "- train the network.\n",
    "- classify validation images and compute accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile main.m\n",
    "tic\n",
    "disp(\"starting the Deep Learning Example\")\n",
    "disp(pwd)\n",
    "\n",
    "digitDatasetPath = fullfile(matlabroot,'toolbox','nnet','nndemos', ...\n",
    "    'nndatasets','DigitDataset');\n",
    "imds = imageDatastore(digitDatasetPath, ...\n",
    "    'IncludeSubfolders',true,'LabelSource','foldernames');\n",
    "\n",
    "disp(\"dataset loaded in memory\")\n",
    "\n",
    "labelCount = countEachLabel(imds)\n",
    "\n",
    "numTrainFiles = 750;\n",
    "[imdsTrain,imdsValidation] = splitEachLabel(imds,numTrainFiles,'randomize');\n",
    "\n",
    "layers = [\n",
    "    imageInputLayer([28 28 1])\n",
    "\n",
    "    convolution2dLayer(3,8,'Padding','same')\n",
    "    batchNormalizationLayer\n",
    "    reluLayer\n",
    "\n",
    "    maxPooling2dLayer(2,'Stride',2)\n",
    "\n",
    "    convolution2dLayer(3,16,'Padding','same')\n",
    "    batchNormalizationLayer\n",
    "    reluLayer\n",
    "\n",
    "    maxPooling2dLayer(2,'Stride',2)\n",
    "\n",
    "    convolution2dLayer(3,32,'Padding','same')\n",
    "    batchNormalizationLayer\n",
    "    reluLayer\n",
    "\n",
    "    fullyConnectedLayer(10)\n",
    "    softmaxLayer\n",
    "    classificationLayer];\n",
    "\n",
    "options = trainingOptions('sgdm', ...\n",
    "    'InitialLearnRate',0.01, ...\n",
    "    'MaxEpochs',4, ...\n",
    "    'Shuffle','every-epoch', ...\n",
    "    'ValidationData',imdsValidation, ...\n",
    "    'ValidationFrequency',30, ...\n",
    "    'Verbose',false, ...\n",
    "    'Plots','training-progress');\n",
    "\n",
    "disp(\"Training started\")\n",
    "\n",
    "net = trainNetwork(imdsTrain,layers,options);\n",
    "\n",
    "disp(\"Training finsished\")\n",
    "\n",
    "YPred = classify(net,imdsValidation);\n",
    "YValidation = imdsValidation.Labels;\n",
    "\n",
    "accuracy = 100*(sum(YPred == YValidation)/numel(YValidation));\n",
    "toc\n",
    "disp(\"Accuracy - \" + string(accuracy))\n",
    "\n",
    "try\n",
    "    fileID = fopen('/opt/ml/processing/output_data/results.txt','w');\n",
    "    disp(fileID)\n",
    "    if fileID==-1\n",
    "        disp(\"cannot open file properly\")\n",
    "    else\n",
    "        fprintf(fileID,'Accuracy - %g\\n', accuracy);\n",
    "        fclose(fileID);\n",
    "    end\n",
    "catch\n",
    "    disp(\"error saving file to output\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Running the docker processing container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at \"Running MATLAB via Processing Job\" section of the README file to understand how to MATLAB via Processing job inside the new processing container.\n",
    "\n",
    "Summarizing the same below:\n",
    "\n",
    "- Uploading `main.m` script file from SageMaker instance to docker processing container.\n",
    "\n",
    "- In section [Dockerfile & dependencies](#3.3-Dockerfile-&-dependencies), a CMD command to run the script `main` located in `/opt/ml/processing/src_files/` directory when the docker container runs.\n",
    "\n",
    "- `main.m` script is uploaded to the `/opt/ml/processing/src_files/` directory of the docker processing container.\n",
    "\n",
    "- Processor gets the `output_data` directory back from the docker processing container as a [ProcessingOutput](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ProcessingOutput) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processor.run(\n",
    "    inputs=\n",
    "    [ProcessingInput(\n",
    "        source='/home/ec2-user/SageMaker/main.m', #location to your main.m script\n",
    "        destination='/opt/ml/processing/src_files/'),\n",
    "    ],\n",
    "    outputs = [\n",
    "        ProcessingOutput(\n",
    "            output_name=\"results\",\n",
    "            source=\"/opt/ml/processing/output_data\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Getting results back from docker processing container to SageMaker instance\n",
    "\n",
    "You can get the `ProcessingOutputConfig` field from latest processing job.\n",
    "\n",
    "In the `main.m` script, you stored accuracy in a `results.txt` file, which is stored in an S3 location. Extract the S3 path of the `results.txt` from the latest processing job, read the contents of the file via [`pd.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function and print the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the output of the latest processing job\n",
    "preprocessing_job_description = processor.jobs[-1].describe()\n",
    "output_config = preprocessing_job_description[\"ProcessingOutputConfig\"]\n",
    "print(output_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the results.txt file from output\n",
    "s3_output_dir = output_config['Outputs'][0]['S3Output']['S3Uri']\n",
    "s3_result = os.path.join(s3_output_dir, \"results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.read_csv(s3_result, header=None)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleaning up resources\n",
    "\n",
    "To avoid incurring unnecessary charges, use the AWS Management Console to delete the resources that you created while running the example.\n",
    "\n",
    "- Open the Amazon S3 console at https://console.aws.amazon.com/s3/, and then delete the bucket that you created for storing model artifacts and the training dataset.\n",
    "- Open the Amazon ECR console at https://console.aws.amazon.com/ecr/, and then delete the repository that you created for storing MATLAB docker image container."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
